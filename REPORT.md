# CS172: Twitter Search Engine: Part A And Part B

Prepared by: Daniel Ma (dma012@ucr.edu), Jeremy Taraba (jtara006@ucr.edu), Neha Gupta (ngupt009@ucr.edu)

## Part A: Build a Web Crawler for edu pages, or a geotagged Twitter post collector

## Collaboration Details PART A

* Daniel: JSON data formatting, URL crawling, concurrency issues.
* Neha: Twitter API setup and abstraction, creating files and output data, Memory Management
* Jeremy: Continuing Twitter API functionality, folder size and file size management, creating files

## Collaboration Details PART B

* Daniel: Lucene insertion/indexing, server backend development
* Neha: Lucene processing, website development, document relevance analysis
* Jeremy: Lucene processing, server backend development, document relevance analysis

## Overview

The Twitter Engine is built on the Tweepy library for Python 3. It collects Tweets
from specified areas and stores them as JSON data.  The data includes the Tweet author,
text, and location Tweeted (if available).  Furthermore, the engine stores the URL and
page title of URLs in the Tweet body.  

The data is broken up into multiple JSON files of configurable size for easy parsing.

The overall system is broken into 3 modules: `crawler.sh` streams realtime tweets from
Twitter; `indexer` indexes these tweets, and a web server searches this index and returns
results to a user.

## Required packages

* Python 3.6+ 
* Python Tweepy 3.5+
* Python lxml
* Python requests 2.18+

## Other required software

* Java 8
* Apache Tomcat 9

## Architecture

The Engine consists of a streamer that continuously collects data from Twitter; an indexer 
that continuously inserts this data into Lucene, and a web server that searches this data
on demand.  All 3 components run independently, though each stage is dependent on the files
generated by the previous stage.

## Index Structures

The tweets are indexed by the username, URL title, tweet text, coordinates, and time submitted.
The time submitted is an integer encoding the year, month, and day submitted, so searches can easily
consider the day and time in the ranking algorithm.

## Limitations

Most Tweets do not come with exact location data, or any location data at all.  This is
because most Twitter users choose not to provide this data to Twitter.

To prevent redudant insertion of data, the indexer can only index data approximately 30 minutes
or so after it is receieved.  This is not a hard limit; this is approximately the time it takes
for the streamer to dump its data to a JSON file and start a new one.

## Usage instructions

Start the streamer with `./crawler.sh`.  Configuration options can be adjusted in `config.json`. The
configuration options are generally self-explanatory, but must be written in valid JSON.

Compile the indexer with `./compile.sh`, and start with `./run_indexer.sh`.  The indexer must be
run in the same directory as `crawler.sh`.  However, it can be run concurrently with `crawler.sh`.

To start the web server, Apache Tomcat is required.  Place the `tomcat_searcher` directory into 
Tomcat's `webapps` directory.  Browse to it via `/tomcat_searcher/search`. Make sure to place the
`index` folder (or symlink it) in the tomcat home directory.

## Screenshots

### JSON output with testing config

![JSON output alongside command running and configuration options][cap1]

### JSON output with testing config 2

![JSON output alongside command running and configuration options][cap2]

### JSON output with testing config 3

![JSON output alongside command running and configuration options][cap3]

### File size variation with testing config

![JSON output alongside command running and configuration options][cap4]

### Program output with "production" config

![Non-testing config options][cap5]


## Part B: Build index and Web-based search interface

### Search with query "covid 19 sucks"
![query 1][cap6]

### Search 2
![query 2][cap7]

### Search with nonsense query
![Nonsense][cap8]

[cap1]: https://github.com/neha45556/geolocatedTweets/raw/master/images/1.png
[cap2]: https://github.com/neha45556/geolocatedTweets/raw/master/images/2.png
[cap3]: https://github.com/neha45556/geolocatedTweets/raw/master/images/3.png
[cap4]: https://github.com/neha45556/geolocatedTweets/raw/master/images/4.png
[cap5]: https://github.com/neha45556/geolocatedTweets/raw/master/images/5.png
[cap6]: https://github.com/neha45556/geolocatedTweets/raw/master/images/6.png
[cap7]: https://github.com/neha45556/geolocatedTweets/raw/master/images/7.png
[cap8]: https://github.com/neha45556/geolocatedTweets/raw/master/images/8.png
